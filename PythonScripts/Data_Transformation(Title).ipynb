{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation of Title Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Translating the data from Polish language to English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from snowflake.sqlalchemy import URL\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.connector.pandas_tools import pd_writer\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the method to access environment variables from .env file\n",
    "# Create the .env file and then initialize variables that holds the following:\n",
    "# Snowflake account name, Snowfalke username, Snowflake password, google account email\n",
    "# Also make sure you copy the service.json file (json file that we get by enabling api in google developer console) in this directory.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for connecting to snowflake database\n",
    "engine = create_engine(URL(\n",
    "                    account = os.getenv('snowflake_account_name'),\n",
    "                    user = os.getenv('snowflake_user_name'),\n",
    "                    password = os.getenv('snowflake_password'),\n",
    "                    database = 'realestate',\n",
    "                    schema = 'public',\n",
    "                    warehouse = 'realestate_wh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A google sheet contains the transformed records will be created.\n",
    "# Along with that log table is also created that wil keep record of the file name for the google sheet.\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        query = \"\"\" SELECT RN, TITLE FROM otodom_data_flatten ORDER BY rn\"\"\"\n",
    "\n",
    "        df = pd.read_sql(query,conn)\n",
    "\n",
    "        gc = gspread.service_account('service_account.json') # json file that we get by enabling api in google developer console\n",
    "\n",
    "        loop_counter = 0\n",
    "        chunk_size = 320000\n",
    "        file_name = 'OTODOM_ANALYSIS_'\n",
    "        user_email = 'example@gmail.com' #google_account_email\n",
    "        for i in range(0,len(df),chunk_size):\n",
    "            loop_counter += 1\n",
    "            df_in = df.iloc[i:(i+chunk_size), :]\n",
    "\n",
    "            spreadsheet_title = file_name + str(loop_counter)\n",
    "            try:\n",
    "                locals()['sh'+str(loop_counter)] = gc.open(spreadsheet_title)\n",
    "            except gspread.SpreadsheetNotFound:\n",
    "                locals()['sh'+str(loop_counter)] = gc.create(spreadsheet_title)\n",
    "\n",
    "            locals()['sh'+str(loop_counter)].share(user_email, perm_type='user', role='writer')\n",
    "            wks = locals()['sh'+str(loop_counter)].get_worksheet(0)\n",
    "            wks.resize(len(df_in)+1)\n",
    "            set_with_dataframe(wks, df_in)   \n",
    "                \n",
    "            column = 'C'   # Column to apply the formula \n",
    "            start_row = 2  # Starting row to apply the formula\n",
    "            end_row = wks.row_count   # Ending row to apply the formula\n",
    "            cell_range = f'{column}{start_row}:{column}{end_row}' \n",
    "            curr_row = start_row\n",
    "            cell_list = wks.range(cell_range)\n",
    "            \n",
    "            for cell in cell_list:\n",
    "                cell.value = f'=GOOGLETRANSLATE(B{curr_row},\"pl\",\"en\")'\n",
    "                curr_row += 1\n",
    "                \n",
    "            # Update the worksheet with the modified cells\n",
    "            wks.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
    "\n",
    "            df_log = pd.DataFrame({'ID':[loop_counter], 'SPREADSHEET_NAME':[spreadsheet_title]})\n",
    "            df_log.to_sql('otodom_data_log', con=engine, if_exists='append', index=False, chunksize=320000, method=pd_writer)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error',e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "engine.dispose()\n",
    "# You can manually download and upload the data in snowflake database or use the below cell for uploading the directly from google sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the file name from log table in snowflake and using it to retrieve the file from google sheets and upload the data to snowflake.\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        query = \"\"\" SELECT ID, SPREADSHEET_NAME FROM otodom_data_log \"\"\"\n",
    "        df = pd.read_sql(query,conn)\n",
    "        df.columns = map(lambda x: str(x).upper(), df.columns)\n",
    "\n",
    "        gc = gspread.service_account('service_account.json')\n",
    "        loop_counter = 0\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            loop_counter += 1\n",
    "            locals()['sh'+str(loop_counter)] = gc.open(row['SPREADSHEET_NAME'])\n",
    "            wks = locals()['sh'+str(loop_counter)].get_worksheet(0)\n",
    "            df_out = get_as_dataframe(wks, usecols=[0,1,2], nrows=wks.row_count, header=None, skiprows=1, evaluate_formulas=True)\n",
    "            \n",
    "            df_out.columns = ['RN', 'TITLE', 'TITLE_ENG']\n",
    "            df_out.to_sql('otodom_data_transformed_title', con=engine, if_exists='append', index=False, chunksize=16000, method=pd_writer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error',e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RealEstateEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
